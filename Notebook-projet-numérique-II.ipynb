{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Projet numérique\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pas fixe :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def solve_euler_explicite(f,x0,dt,t0,tf):\n",
    "    T=[t0]\n",
    "    X=[x0]\n",
    "    t=t0\n",
    "    x=x0\n",
    "    n = int((tf-t0)/dt) # nombre de pas à faire entre t0 et tf\n",
    "    for i in range(n) :\n",
    "        dx = f(x,t)*dt\n",
    "        x += dx\n",
    "        t += dt\n",
    "        T.append(t)\n",
    "        X.append(x)\n",
    "    return T,X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La fonction $solve\\_euler\\_explicite$ utilise la méthode numérique d'Euler pour obtenir une solution approchée de la solution de l'équation différentielle $x'(t)=f(x,t)$, telle que $x(t_0)=x_0$. C'est une méthode d'ordre 1 : l'erreur commise sur un intervalle de taille $(t_f-t_0)$ est en $O(dt)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def solve_heun(f,x0,dt,t0,tf):\n",
    "    T=[t0]\n",
    "    X=[x0]\n",
    "    t=t0\n",
    "    x=x0\n",
    "    n = int((tf-t0)/dt)\n",
    "    for i in range(n) :\n",
    "        dx = dt/2*(f(x,t)+f(x+dt*f(x,t),t+dt)) # on utilise la méthode de Heun pour obtenir une approximation de x(j+1) à l'ordre 2\n",
    "        x += dx\n",
    "        t += dt\n",
    "        T.append(t)\n",
    "        X.append(x)\n",
    "    return T,X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La méthode de Heun utilisée dans la fonction précédente est d'ordre 2 : l'erreur est en $O(dt^2)$. Cet algorithme est donc bien plus efficace, car il limite grandement l'erreur sur la solution, sans trop augmenter les temps de calcul."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On peut comparer ces deux algorithmes en résolvant une équation différentielle dont on connait une solution exacte, et en utilisant différents pas de temps.\n",
    "On prend l'exemple de $f(x,t)=1/x$, de solution $g(t)=\\sqrt{1+2t}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "f = lambda x,t : 1/x # fonction telle que dx/dt = f(x,t)\n",
    "\n",
    "g = lambda t : np.sqrt(1+2*t) # solution g de l'équation différentielle\n",
    "\n",
    "def test_ecart(f,g,x0,dt,t0,tf): # renvoie les écarts maximaux entre la solution exacte g et les solutions approchées par Euler et Heun\n",
    "    T,X1=solve_euler_explicite(f,x0,dt,t0,tf)\n",
    "    T,X2=solve_heun(f,x0,dt,t0,tf)\n",
    "    Y = [g(t) for t in T]\n",
    "    L_ecart_euler = np.abs(np.array(Y)-np.array(X1))\n",
    "    L_ecart_heun = np.abs(np.array(Y)-np.array(X2))\n",
    "    ecart_euler = np.max(L_ecart_euler)\n",
    "    ecart_heun = np.max(L_ecart_heun)\n",
    "    return ecart_euler,ecart_heun\n",
    "\n",
    "x0=1\n",
    "t0=0\n",
    "tf = 10\n",
    "\n",
    "L_dt = [1,0.9,0.8,0.7,0.6,0.5,0.4,0.3,0.1,0.05,0.025]\n",
    "Euler = []\n",
    "Heun = []\n",
    "for dt in L_dt :\n",
    "    e1,e2 = test_ecart(f,g,x0,dt,t0,tf)\n",
    "    Euler.append(e1)\n",
    "    Heun.append(e2)\n",
    "\n",
    "plt.plot(L_dt,Euler,'r',label='Euler')\n",
    "plt.plot(L_dt,Heun,'g',label='Heun')\n",
    "plt.legend()\n",
    "plt.xlabel('dt')\n",
    "plt.ylabel('erreur')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T,X1=solve_euler_explicite(f,x0,0.5,t0,tf)\n",
    "T,X2=solve_heun(f,x0,0.5,t0,tf)\n",
    "Y = [g(t) for t in T]\n",
    "plt.plot(T,X1,'r',label='Euler')\n",
    "plt.plot(T,X2,'g',label='Heun')\n",
    "plt.plot(T,Y,'b',label='solution exacte')\n",
    "plt.legend()\n",
    "plt.xlabel('t')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On voit que pour $dt$ suffisament petit, les erreurs commises par les méthodes d'Euler et de Heun tendent vers 0. Cependant, on observe que l'erreur croit beaucoup plus rapidement pour la méthode d'Euler que pour la méthode de Heun. Ainsi, pour un pas de temps $dt = 0.5$, la solution exacte et la solution approchée par Heun sont presque confondues, alors qu'on observe un écart non-négligeable avec la solution approchée d'Euler.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les méthodes présentées ci-dessus utilisent un pas de temps constant. Cependant certains systèmes ne peuvent être résolus avec un pas fixe (comme par exemple les systèmes dits raides). Pour remédier à ce problème il est possible d'utiliser des pas variables qui vont permettre d'adapter le pas de temps au cours de la résolution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pas variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\t\n",
    "On commence par s'interesser à l'erreur absolue locale que l'on souhaite estimer. Cette estimation de l'erreur commise permet de choisir un pas de temps plus adapté (en diminuant le pas de temps si l'erreur est trop élevée ou en l'augmentant si elle est bien au dessous de la tolérance voulue). On cherche dans un premier temps à exprimer $e^{j+1}$ qui représente cette erreur.\n",
    "\t\n",
    "$$e^{j+1}=x(t_j)+\\int_{t_j}^{t_{j+1}} f(s,x(s))ds-x^{j+1}$$\n",
    "\n",
    "\n",
    "or $$f(s,x(s))=x'(s)$$ \n",
    "donc $$\\int_{t_j}^{t_{j+1}} f(s,x(s))ds=x(t_{j+1})-x(t_j)$$\n",
    "et $$e^{j+1}= x(t_{j+1})-x^{j+1}$$\n",
    "\n",
    "On suppose que l'on a $f$ $C^1$ donc $x$ est $C^2$, en faisant un développement limité au second ordre on obtient :\n",
    "\n",
    "$$x(t_{j+1})=x(t_j)+ \\Delta t f(t_j,x(t_j))+\\frac{\\Delta t^2}{2} [\\partial_t{f(t_j,x(t_j))}+\\partial_x{f(t_j,x(t_j))}f(t_j,x(t_j))]+O(\\Delta t^3)$$\n",
    "\n",
    "et de même en faisant un développement limité de f :\n",
    "\n",
    "$$f(x_{j+1},x(t_{j+1}))=f(t_j,x(t_j))+\\Delta t [\\partial_t{f(t_j,x(t_j)}+\\partial_x{f(t_j,x(t_j))}f(t_j,x(t_j))] + O(\\Delta t^2)$$\n",
    "\n",
    "Développons l'expression de $e^{j+1}$ en se servant du fait que l'on utilise une méthode d'Euler explicite.\n",
    "\n",
    "Remarque : On notera indifférement $x^j$ et $x(t_j)$ qui représentent ici la même chose.\n",
    "\n",
    "$$\\begin{eqnarray*}\n",
    "e^{j+1} & = & x(t_{j+1})-x^{j+1}  \\\\\n",
    "& = & x(t_j)+ \\Delta t f(t_j,x(t_j))+\\frac{\\Delta t^2}{2} (\\partial_t{f(t_j,x(t_j)}+\\partial_x{f(t_j,x(t_j)}f(t_j,x(t_j)) \\\\\n",
    "&  & -(x^j + \\Delta t f(t_j,x(t_j))) + O(\\Delta t^3) \\\\\n",
    "&=& \\frac{\\Delta t^2}{2} [\\partial_t{f(t_j,x(t_j)}+\\partial_x{f(t_j,x(t_j))}f(t_j,x(t_j))] + O(\\Delta t^3) \\\\\n",
    "\\end{eqnarray*}$$\n",
    "\n",
    "\n",
    "\n",
    "D'un autre côté on a :\n",
    "\n",
    "$$\\begin{eqnarray*}\n",
    "f(t_{j+1},x^{j+1}) & = & f(t_j+\\Delta t,x^j+\\Delta t f(t_j,x^j) )  \\\\ \n",
    "& = & f(t_j,x^j) + \\Delta t [\\partial_t{f(t_j,x(t_j))}+\\partial_x{f(t_j,x(t_j))}f(t_j,x(t_j))] + O(\\Delta t^2) \\\\\t\n",
    "\\end{eqnarray*}$$\n",
    "\n",
    "donc :\n",
    "$$\\frac{\\Delta t}{2}[f(t_{j+1},x^{j+1})-f(t_j,x^j)] = \\frac{\\Delta t^2}{2} [\\partial_t{f(t_j,x(t_j))}+\\partial_x{f(t_j,x(t_j))}f(t_j,x(t_j))] + O(\\Delta t^3) $$\n",
    "On déduit donc que :\n",
    "$$e^{j+1} = \\frac{\\Delta t}{2}[f(x_{j+1},x(t_{j+1}))-f(t_j,x^j)] + O(\\Delta t^3) $$\n",
    "finalement :\n",
    "$$||e^{j+1}||=\\frac{\\Delta t}{2}||f(t_{j+1},x^{j+1})-f(t_j,x^j)|| + O(\\Delta t^3) $$\n",
    "\t\n",
    "\t\n",
    "\t\n",
    "\t\n",
    "Notre but est de pouvoir adapter le pas de temps tout en acceptant une tolérance $Tol_{abs}$ sur l'erreur.\n",
    "Au cours de notre réflexion nous avons vu deux stratégies d'adaptation possibles : \n",
    "\t\n",
    "Dans la première stratégie, nous estimons le pas de temps $j+2$ en se basant sur le pas de temps $j+1$. Pour cela nous corrigeons le pas en $j+1$ grâce à une évaluation de l'erreur commise en $j+1$. Cette stratégie nécessite que $x$ soit suffisement régulière afin que deux pas de temps \"optimaux\" successifs ne soient pas trop différents.\n",
    "\t\n",
    "Une seconde stratégie consiste à prendre un pas de temps (choisi à l'avance ou égal au précedent) et à évaluer l'erreur que l'on commet. Grâce à l'évaluation de l'erreur on déduit un nouveau pas de temps puis refait le calcul de $x^{j+1}$ avec le nouveau pas. Cette stratégie nécessite donc de calculer chaque point deux fois mais semble plus robuste car ne necessite moins d'hypothèses.\n",
    "\t\n",
    "Ces deux méthodes nécessitent d'évaluer $e^{j+1}$. Cela est possible avec la formule montrée précédemment. On donne cependant une autre expression plus simple de l'erreur facilement transposable dans un algorithme.\n",
    "Pour cela on calcule $x^{j+1}$ avec la méthode numérique puis on calcule un $x_{new}^j$ en revenant sur nos pas avec la méthode numérique toujours. On a alors :\n",
    "\t\n",
    "$$x_{new}^j=x^{j+1}-\\Delta t f(t_{j+1},x^{j+1})$$\n",
    "\t\n",
    "donc\n",
    "\t\n",
    "$$\\begin{eqnarray*}\n",
    "||x_{new}^j-x^j||& = & ||x^{j+1}-\\Delta t f(t_{j+1},x^{j+1})-x^j|| \\\\\n",
    "& = & ||x^j + \\Delta t f(t_j,x^j)-\\Delta t f(t_{j+1},x^{j+1})-x^j|| \\\\\n",
    "& = & \\Delta t ||f(t_{j+1},x^{j+1})-f(t_j,x^j)||\n",
    "\\end{eqnarray*}$$\n",
    "\n",
    "On déduit donc que \n",
    "$$||e^{j+1}||= ||\\frac{x_{new}^j-x^j}{2}|| +O(\\Delta t^3)$$\n",
    "\t\t\n",
    "Cette dernière formule nous donne une évaluation de l'erreur. A partir de l'erreur et de la tolérance souhaitée on calcule le nouveau pas de temps. Pour cela on sait que $e^{j+1}=O(\\Delta t^2)$ car :\n",
    "$$e^{j+1}= \\frac{\\Delta t^2}{2} [\\partial_t{f(t_j,x(t_j))}+\\partial_x{f(t_j,x(t_j))}f(t_j,x(t_j))] + O(\\Delta t^3)$$\n",
    "donc :\n",
    "$$e^{j+1)}=O(\\Delta t^2)$$\n",
    "On comprends alors qu'une diminution quadratique de $\\Delta t$ mène à une diminution au première ordre de $e^{j+1}$. En effet $\\frac{e^{j+1}}{\\Delta t^2}$ est constant dans une première approximation, on obtient donc :\n",
    "$$\\frac{||e^{j+1}||}{\\Delta t^2}=\\frac{Tol_abs}{\\Delta t_{new}^2}$$\n",
    "On déduit alors que l'on peut prendre un nouveau pas\n",
    "\t\n",
    "$$\\Delta t_{new}=\\Delta t \\sqrt{\\frac{Tol_{abs}}{||e^{j+1}||}}$$\t\n",
    "\n",
    "La fonction ci-dessous met en application la première stratègie évoquée ci-dessus.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def solve_ivp_euler_explicit_variable_step(f, t0, x0, t_f, dtmin = 1e-16, dtmax = 0.01, atol = 1e-6):\n",
    "    dt = dtmax/10; # initial integration step\n",
    "    ts, xs = [t0], [x0]  # storage variables\n",
    "    t = t0\n",
    "    ti = 0  # internal time keeping track of time since latest storage point : must remain below dtmax\n",
    "    x = x0\n",
    "    while ts[-1] < t_f:\n",
    "        while ti < dtmax: # on ne veut pas que le temps entre deux enregistrements dépasse dtmax\n",
    "            t_next, ti_next, x_next = t + dt, ti + dt, x + dt * f(x) # on avance dans le temps de dt, on modifie t, ti et xi en conséquence\n",
    "            x_back = x_next - dt * f(x_next) # on calcule le x précédent en appliquant Euler à x_next : le résultat est légèrement différent de x (voir dessin)\n",
    "            ratio_abs_error = atol / (np.linalg.norm(x_back-x)/2) # on calcule l'erreur commise\n",
    "            dt = 0.9 * dt * np.sqrt(ratio_abs_error) # on calcule le nouveau pas de temps avec la formule calculée précédement (le 0.9 représente la marge de sécurité)\n",
    "            if dt < dtmin: # si le pas de temps est trop faible, les erreurs d'arrondi vont fausser le résultat : on arrête la simulation\n",
    "                raise ValueError(\"Time step below minimum\")\n",
    "            elif dt > dtmax/2: # on empèche le pas de temps d'être trop grand (l'ensemble des calculs précédents ont été faits en supposant dt \"petit\"...)\n",
    "                dt = dtmax/2\n",
    "            t, ti, x = t_next, ti_next, x_next\n",
    "        dt2DT = dtmax - ti # time left to dtmax ; dt2DT est négatif (à cause de la condition de sortie de boucle)\n",
    "        t_next, ti_next, x_next = t + dt2DT, 0, x + dt2DT * f(x) # on se replace au temps dtmax, on modifie x en conséquence et on réinitialise la valeur de ti\n",
    "        ts = np.vstack([ts,t_next])\n",
    "        xs = np.vstack([xs,x_next])\n",
    "        t, ti, x = t_next, ti_next, x_next\n",
    "    return (ts, xs.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La fonction solve_ivp_euler_explicit_variable_step permet d'appliquer la méthode d'Euler, en optimisant le pas de temps à chaque étape, afin que l'erreur commise à chaque pas soit proche d'un certain seuil $atol$. Afin que les résultats soient facilement exploitables et plus légers, l'algorithme ne renvoie les valeurs de la fonction qu'avec une période constante $dt_{max}$ que l'on peut voir comme le temps d'échantillonnage de la fonction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "omega=1\n",
    "def f(xy):\n",
    "    x, y = xy\n",
    "    return np.array([y, -omega*x])\n",
    "\n",
    "t0, tf, x0 = 0.0, 30.0, np.array([-1.0, 0.0])\n",
    "\n",
    "t, x = solve_ivp_euler_explicit_variable_step(f, t0, x0, tf)\n",
    "\n",
    "X=[-np.cos(i) for i in t]\n",
    "Y=[np.sin(i) for i in t]\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(t, x[0],'r') # solution approchée en rouge\n",
    "plt.plot(t, x[1],'r')\n",
    "plt.plot(t,X,'g') # solution réelle en vert\n",
    "plt.plot(t,Y,'g')\n",
    "plt.grid(True)\n",
    "plt.figure()\n",
    "plt.plot(x[0],x[1],'r')\n",
    "plt.plot(X,Y,'g') # tracé du cercle de centre à l'origine et de rayon 1\n",
    "plt.plot([0],[0],'.')\n",
    "plt.axis('equal')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lorsque l'on utilise cette fonction pour le système $x'(t)=y(t)$ et $y'(t)=-x(t)$ (de solutions réelles $x(t)=-cos(t)$ et $y(t)=sin(t)$), on remarque que la solution approchée tend à diverger. La quantitée $x^2+y^2$ n'est pas conservée. On retrouve donc le même défaut que la méthode d'Euler classique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(xabc):\n",
    "    xa, xb, xc = xabc\n",
    "    return np.array([-0.04 * xb + 1e4 * xb * xc, 0.04 * xa - 1e4 * xb * xc - 3e7 * xb**2,  3e7 * xb**2])\n",
    "\n",
    "t0, tf, x0 = 0.0, 5.0, np.array([1.0, 0.0, 0.0])\n",
    "\n",
    "t, x = solve_ivp_euler_explicit_variable_step(f, t0, x0, tf, dtmin = 1e-20, atol = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'exemple précédent montre que cet algorithme ne fonctionne pas lorsque la solution prend des valeurs trop grandes et varie trop rapidement. En effet, d'importantes variations augmentent l'erreur commise et requièrent donc l'utilisation d'un pas de temps plus faible. La limite $dt_{min}$ risque alors d'être atteinte. Cet algorithme n'assure alors plus de précision suffisante à cause des erreurs d'arrondi."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
